{
	"name": "TrainUnsupervisedModel - With Visuals",
	"properties": {
		"folder": {
			"name": "SparkMLServices"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "sparkpool2021",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "56g",
			"driverCores": 8,
			"executorMemory": "56g",
			"executorCores": 8,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "be004374-4a49-42a0-a7ee-da55eead7e19"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/03766bec-6c31-4851-aa3c-233d9b60aeaf/resourceGroups/uiap-d-si-synapse/providers/Microsoft.Synapse/workspaces/uiap-synapse-claims-ws/bigDataPools/sparkpool2021",
				"name": "sparkpool2021",
				"type": "Spark",
				"endpoint": "https://uiap-synapse-claims-ws.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparkpool2021",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 3,
				"cores": 8,
				"memory": 56,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"### Health Insurance Fraud Claim Detection using Unsupervised Machine Learning !"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"<p>\n",
					"Health insurance fraud can be explained as a situation where an insured or medical service provider furnishes fraud, false or misleading information to the insurer with the intention to attain unfair benefits from a policy for the policy holder or service providing source.\n",
					"\n",
					"Such fraud leads to serious losses for the insurance service providers but it could also result in impacting the health insurance advantage for genuine customers. Also, “semi-urban and rural areas” have witnessed more cases of health insurance as compared to metro cities.\n",
					"</p>"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"<b>Most common health insurance frauds:</b>\n",
					"    \n",
					"It is a difficult task to explain the types of health insurance frauds as the nature of deceit keeps changing over the years. The most common types of frauds are listed below;\n",
					"\n",
					"<li>Filing a claim for treatments or services that were never administered. This is often done by forging genuine patient information and manufacturing admission in connivance with service providers.</li>\n",
					"\n",
					"<li>Increasing overall cost of hospitalisation by including treatments that were not necessary basis the medical problem.</li>\n",
					"\n",
					"<li>Misrepresenting treatments that are not covered as medically necessary.</li>\n",
					"\n",
					"<li>Non-disclosure of Pre-Existing Diseases and manufacturing diagnosis reports to justify tests, examinations and surgeries to prove claim worthiness.</li>"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"<b>Dataset Information :</b>\n",
					"<hr/>\n",
					"<li>Claim FIR No: Claim First Investigation Report Number</li>\n",
					"<li>Claim Type : Claim Categoreies</li>\n",
					"<li>Hospitalization Type: Mode of Hospitalization</li>\n",
					"<li>Class_of_Accommodation: Client Room Accommodation</li>\n",
					"<li>Procedure: Doctor Summary Report</li>\n",
					"<li>Diagnosis: Medical Diagnosis Report</li>\n",
					"<li>policyNo: Claim Policy Number</li>\n",
					"<li>Policy Type: Client Policy Type</li>\n",
					"<li>member_code: CLient Member Code</li>\n",
					"<li>FullMemberName: Name of the Client</li>\n",
					"<li>Gender: Gender</li>\n",
					"<li>Age: Age</li>\n",
					"<li>Hospital: Client Admitted Hospitals</li>\n",
					"<li>Provider Network Type: Client Network provider</li>\n",
					"<li>Corporate: Client Coporate</li>\n",
					"<li>Length of Stay: Length of Stay in the hospital</li>\n",
					"<li>IsFraudClaim: Type of Fraud Cliam</li>\n",
					"<li>Fraud Score: Fraud Accuracy Score</li>\n",
					"<li>Claim Amount: Cliam Amount</li>\n",
					"<li>Claim Date: Claim Date</li>"
				]
			},
			{
				"cell_type": "markdown",
				"source": [
					"##### Importing Libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import pandas as pd\n",
					"import numpy as np\n",
					"from matplotlib import pyplot as plt\n",
					"import seaborn as sns\n",
					"from pyspark.sql.functions import * #isnan, when, count, col, regexp_replace, udf\n",
					"from pyspark.sql.types import StringType,IntegerType\n",
					"\n",
					"import nltk\n",
					"import re\n",
					"nltk.download('punkt')\n",
					"nltk.download('stopwords')\n",
					"\n",
					"from nltk.corpus import stopwords\n",
					"from nltk.tokenize import word_tokenize\n",
					"\n",
					"from sklearn.preprocessing import LabelEncoder\n",
					"from sklearn.feature_extraction.text import CountVectorizer\n",
					"from sklearn.cluster import KMeans\n",
					"from sklearn.ensemble import IsolationForest\n",
					"from sklearn.preprocessing import StandardScaler\n",
					"from sklearn.decomposition import PCA\n",
					"\n",
					"%matplotlib inline\n",
					"sns.set()\n",
					"plt.rcParams[\"figure.figsize\"] = (15,8)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"### Credentials\n",
					"\n",
					"driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
					"url = dbutils.secrets.get(scope = \"ai-ml-workathon-scope\", key = \"SQLTargetJdbc\")\n",
					"table = \"dbo.ClaimsSummary_ModelInput\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df = spark.read.format(\"jdbc\")\\\n",
					"  .option(\"driver\", driver)\\\n",
					"  .option(\"url\", url)\\\n",
					"  .option(\"dbtable\", table)\\\n",
					"  .load()\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df.count()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df.show(3,False)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"print(df.columns)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df = df.withColumnRenamed(\"ClaimFIRNo\",\"Claim_FIR_No\") \\\n",
					"    .withColumnRenamed(\"ClaimType\",\"Claim_Type\") \\\n",
					"    .withColumnRenamed(\"HospitalizationType\",\"Hospitalization_Type\") \\\n",
					"    .withColumnRenamed(\"ClassofAccommodation\",\"Class_of_Accommodation\") \\\n",
					"    .withColumnRenamed(\"PolicyType\",\"Policy_Type\") \\\n",
					"    .withColumnRenamed(\"LengthofStay\",\"Length_of_Stay\") \\\n",
					"    .withColumnRenamed(\"ClaimAmount\",\"Claim_Amount\") \\\n",
					"    .withColumnRenamed(\"ClaimDate\",\"Claim_Date\")    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"df = df.na.fill(0)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"strcol = ['Claim_Type', 'Hospitalization_Type', 'Class_of_Accommodation','Procedure', 'Diagnosis', 'PolicyNo', 'Policy_Type', 'member_code','FullMemberName','Gender','Hospital','Corporate','HospitalCity','HospitalState','MemberCity','MemberState']\n",
					"\n",
					"for i in strcol:\n",
					"  df = df.withColumn(i, regexp_replace(i, '0', 'NO'))\n",
					"  df = df.withColumn(i, regexp_replace(i, 'NULL', 'NO'))\n",
					"  df = df.withColumn(i, regexp_replace(i, 'Null', 'NO'))\n",
					"  df = df.withColumn(i, regexp_replace(i, 'null', 'NO'))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"upperCaseUDF = udf(lambda z:str(z).upper())\n",
					"  \n",
					"for i in [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]:\n",
					"  df=df.withColumn(i, upperCaseUDF(col(i)))   \n",
					"  \n",
					"def preunique(x):    \n",
					"    if x.find('DAYCARE')>=0 or x.find('DAY CARE')>=0:\n",
					"        return 'DAY CARE'\n",
					"    if x.find('ICCU')>=0 or x.find('ICU')>=0 or x.find('GW & ICU')>=0 or  x.find('PICU')>=0 or x.find('ICU/SPL')>=0 or  x.find('ICU & Deluxe')>=0:\n",
					"        return 'ICCU'\n",
					"    if x.find(\"SEMI PVT\")>=0 or x.find(\"SEMI SPECIAL\")>=0 or x.find(\"S PVT\")>=0 or x.find(\"SEMI\")>=0 or x.find(\"S PRIVATE\")>=0 or x.find(\"PRIVATE\")>=0 or x.find(\"PVT ROOM\")>=0 or x.find(\"PVT A/C\")>=0 or x.find(\"PVt AC\")>=0 or x.find(\"PVT\")>=0:\n",
					"        return 'PRIVATE'\n",
					"    if x.find(\"GW\")>=0 or x.find(\"GENERAL\")>=0 or x.find(\"GENERAL WARD\")>=0 or x.find(\"G/W\")>=0:\n",
					"        return 'GENERAL WARD'\n",
					"    if x.find(\"DELUXE\")>=0 or x.find(\"DELUX\")>=0 or x.find(\"DLX\")>=0:\n",
					"        return 'DELUX'\n",
					"    if x.find(\"HEALTH CHECK UP\")>=0 or x.find(\"HEALTH CHECKUP\")>=0 or x.find(\"CHECKUP\")>=0:\n",
					"        return 'HEALTH CHECKUP'\n",
					"    if x.find(\"PRE\")>=0 or x.find(\"POST\")>=0 or x.find(\"PRE POST\")>=0 or x.find(\"PRE & POST\")>=0 or x.find(\"PREPOST\")>=0:\n",
					"        return 'PRE POST'\n",
					"    if x.find(\"TWIN\")>=0 or x.find(\"TWING\")>=0 or x.find(\"TWIN SHARING\")>=0 or x.find(\"TWING SHARING\")>=0 or x.find(\"TWIN + CLASSIC\")>=0 or x.find(\"SHARING\")>=0:\n",
					"        return 'TWIN SHARING'\n",
					"    if x.find(\"NON AC\")>=0 or x.find(\"NON A/C\")>=0 or x.find(\"NON-A/C\")>=0 or x.find(\"NON-AC\")>=0:\n",
					"        return 'NON AC ROOM'\n",
					"    if x.find(\"AC\")>=0 or x.find(\"A/C\")>=0 or x.find(\"SPECIAL\")>=0 or x.find(\"STANDARD\")>=0:\n",
					"        return 'AC ROOM'\n",
					"    if x.find(\"SINGLE\")>=0:\n",
					"        return 'SINGLE ROOM'    \n",
					"    if x.find(\"NO\")>=0 or x.find(\"NA\")>=0:\n",
					"        return 'NO'\n",
					"    if x.find(\"SECOND\")>=0 or x.find(\"DOUBLE\")>=0:\n",
					"        return 'DOUBLE ROOM'     \n",
					"    if x.find('ROOM')>=0:\n",
					"        return 'ROOM'\n",
					"    if x.find('RENT')>=0:\n",
					"        return 'RENT' \n",
					"    return x\n",
					"  \n",
					"clsofAccUDF = udf(lambda z : preunique(z), StringType()) \n",
					"df = df.withColumn('Class_of_Accommodation', clsofAccUDF(col('Class_of_Accommodation')))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf = df.toPandas()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf.head().T"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf.dtypes"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def textclean(txt):    \n",
					"    token=word_tokenize(txt)\n",
					"    punc=[word for word in token if word.isalpha()]\n",
					"    mystop=set(stopwords.words('english'))\n",
					"    new_tok=[]\n",
					"    for tok in punc:\n",
					"        if tok not in mystop:\n",
					"            new_tok.append(tok)\n",
					"    return \" \".join(new_tok)    \n",
					"  \n",
					"for i in ['Procedure','Diagnosis']:\n",
					"  pdf[i]=pdf[i].apply(lambda x: textclean(x))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf.isna().sum()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf.loc[pdf['Age']=='NONE','Age']=30\n",
					"pdf.loc[pdf['Claim_Amount']==0,'Claim_Amount']=int(pdf['Claim_Amount'].mean())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf['Claim_Amount']=pdf['Claim_Amount'].apply(lambda x: int(x))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf[['Age', 'Claim_Amount']] = pdf[['Age', 'Claim_Amount']].apply(pd.to_numeric)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"len(pdf)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"len(pdf['Claim_FIR_No'].unique())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf=pdf.drop_duplicates(subset=['Claim_FIR_No'])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"def bucketize(val, size, count):\n",
					"    i=0\n",
					"    for i in range(count):\n",
					"        if val <= (i+1)*size:\n",
					"            return i\n",
					"    return i\n",
					"\n",
					"pdf['Age_group'] = [bucketize(x, 10, 5) for x in pdf['Age']]    \n",
					"pdf['Claim_amt_grp'] = [bucketize(x, 1000, 200) for x in pdf['Claim_Amount']]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf['day'] = pd.DatetimeIndex(pdf['Claim_Date']).day\n",
					"pdf['year'] = pd.DatetimeIndex(pdf['Claim_Date']).year\n",
					"pdf['month'] = pd.DatetimeIndex(pdf['Claim_Date']).month"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"pdf.columns"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# 'Claim_FIR_No'\n",
					"dt=pdf.drop(['FullMemberName','Claim_Date','ProcedureAmount'],axis=1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dt[['day', 'month', 'year','Length_of_Stay','Claim_Amount']] = dt[['day', 'month', 'year','Length_of_Stay','Claim_Amount']].apply(pd.to_numeric)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"##### Selecting Column Datatype"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dt.select_dtypes(include=['object']).columns"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dt.select_dtypes(include=['int32']).columns"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dt.select_dtypes(include=['int64']).columns"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dt.select_dtypes(include=['float64']).columns"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"ddf = dt.copy()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"ddf.shape"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"sns.heatmap(ddf.corr())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"print(\"Unique Records ->\",\"\\nMember Code : \",len(dt['member_code'].unique()),\"\\nPolicy No : \",len(dt['PolicyNo'].unique()),\"\\nPolicy Type :  \", len(dt['Policy_Type'].unique()))"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"##### Convert String Data into Numeric Data by LabelEncoder"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"le = LabelEncoder()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"col_n=['Claim_Type', 'Hospitalization_Type', 'Class_of_Accommodation', 'PolicyNo', 'Policy_Type','member_code',\n",
					"       'Gender', 'Hospital', 'Corporate','HospitalCity', 'HospitalState', 'MemberCity','MemberState']"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"for i in col_n:\n",
					"    ddf[i]=le.fit_transform(ddf[i])"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"ddf[['Diagnosis','Procedure']].head()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"c1 = CountVectorizer()\n",
					"c2 = CountVectorizer()\n",
					"\n",
					"dia = c1.fit_transform(ddf['Diagnosis'])\n",
					"pro = c2.fit_transform(ddf['Procedure'])\n",
					"\n",
					"diag = pd.DataFrame(dia.toarray(), columns=c1.get_feature_names())\n",
					"prod = pd.DataFrame(pro.toarray(), columns=c2.get_feature_names())\n",
					"\n",
					"dft=pd.concat([diag,prod],axis=1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dfr=ddf.drop(['Claim_FIR_No','Diagnosis','Procedure'], axis=1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"#### Now Our Dataset is ready for proceed for ML"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"dfr.head()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dft.head()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"sns.heatmap(dfr.corr(),cmap='YlGnBu')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"fig, ax = plt.subplots(nrows=7,ncols=3, figsize=(20,10), constrained_layout=True)\n",
					"plt.suptitle(\"Uni-Variate Outlier Data Analyis\")\n",
					"ax=ax.flatten()\n",
					"for x, i in enumerate(dfr):\n",
					"    sns.boxplot(data=dfr,x=i, ax=ax[x], color='#51A2DB')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# fig, ax = plt.subplots(nrows=7,ncols=3, figsize=(20,20), constrained_layout=True)\n",
					"# ax=ax.flatten()\n",
					"# int_cols= dfr\n",
					"# for x, i in enumerate(int_cols):    \n",
					"#     sns.histplot(data=dfr,x=i,color='m', ax=ax[x],kde=True)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"iforest_d = IsolationForest(n_estimators=100, max_samples='auto',contamination=0.05, max_features=1.0,bootstrap=False, n_jobs=-1, random_state=0)\n",
					"iforest_t = IsolationForest(n_estimators=100, max_samples='auto',contamination=0.05, max_features=1.0,bootstrap=False, n_jobs=-1, random_state=0)\n",
					"\n",
					"X_d=dfr.values\n",
					"X_t=dft.values\n",
					"\n",
					"pred_d = iforest_d.fit_predict(X_d)\n",
					"pred_t = iforest_t.fit_predict(X_t)\n",
					"\n",
					"scaler_d = StandardScaler()\n",
					"scaler_t = StandardScaler()\n",
					"\n",
					"ddfs=scaler_d.fit_transform(X_d)\n",
					"dtfs=scaler_t.fit_transform(X_t)\n",
					"\n",
					"# Apply PCA to reduce the dimensionality\n",
					"pca_d = PCA(n_components=2)\n",
					"pca_t = PCA(n_components=2)\n",
					"\n",
					"pcd = pca_d.fit_transform(ddfs)\n",
					"pct = pca_t.fit_transform(dtfs)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"# Extract outliers\n",
					"outlier_index_d = np.where(pred_d==-1)\n",
					"outlier_index_t = np.where(pred_t==-1)\n",
					"\n",
					"outlier_values_d = X_d[outlier_index_d]\n",
					"outlier_values_t = X_t[outlier_index_t]\n",
					"\n",
					"outlier_values_scaled_d = scaler_d.transform(outlier_values_d)\n",
					"outlier_values_scaled_t = scaler_t.transform(outlier_values_t)\n",
					"\n",
					"outlier_values_pca_d = pca_d.transform(outlier_values_scaled_d)\n",
					"outlier_values_pca_t = pca_t.transform(outlier_values_scaled_t)\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Plot the data\n",
					"sns.scatterplot(x=pcd[:,0], y=pcd[:,1])\n",
					"sns.scatterplot(x=outlier_values_pca_d[:,0], y=outlier_values_pca_d[:,1], color='red')\n",
					"plt.title(\"Isolation Forest Outlier Detection - Numeric Values\",fontsize=15, pad=15)\n",
					"plt.xlabel(\"PC1\")\n",
					"plt.xlabel(\"PC2\")\n",
					"plt.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"(pred_t==-1).sum()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Plot the data\n",
					"sns.scatterplot(x=pct[:,0], y=pct[:,1])\n",
					"sns.scatterplot(x=outlier_values_pca_t[:,0], y=outlier_values_pca_t[:,1], color='red')\n",
					"plt.title(\"Isolation Forest Outlier Detection - Text Values\",fontsize=15, pad=15)\n",
					"plt.xlabel(\"PC1\")\n",
					"plt.xlabel(\"PC2\")\n",
					"plt.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Save the Anomaly Result \n",
					"\n",
					"dtest=dt.copy()\n",
					"dtest['AD_D']=pred_d\n",
					"dtest['AD_T']=pred_t"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"#### Total Outlier Detection"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"print(\"The Length of Anomaly Detection :\\n\\nNumerical Values  - \",len(dtest[dtest['AD_D']==-1]),\"\\nTextical Values   - \",len(dtest[dtest['AD_T']==-1]))"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dtest[dtest['AD_D']==-1].tail().T"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dtest[dtest['AD_T']==-1].head().T"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"##### Direct KMeans Clustering with PCA Transformation"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"kmeans_d = KMeans(n_clusters=100, init='k-means++', n_init=100, max_iter=100, random_state=0)\n",
					"kmeans_d.fit(pcd)\n",
					"dtest['C1_KM']=kmeans_d.labels_\n",
					"\n",
					"kmeans_t = KMeans(n_clusters=100, init='k-means++', n_init=100, max_iter=100, random_state=0)\n",
					"kmeans_t.fit(pct)\n",
					"dtest['C2_KM']=kmeans_t.labels_"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"source": [
					"##### K-Means Clustering"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"plt.scatter(pcd[:,0],pcd[:,1],c=kmeans_d.labels_,cmap='rainbow',marker='o')\n",
					"plt.colorbar()\n",
					"plt.scatter(kmeans_d.cluster_centers_[:,0],kmeans_d.cluster_centers_[:,1],color='black',marker='+',s=200)\n",
					"plt.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"plt.scatter(pct[:,0],pct[:,1],c=kmeans_t.labels_,cmap='rainbow',marker='o')\n",
					"plt.colorbar()\n",
					"plt.scatter(kmeans_t.cluster_centers_[:,0],kmeans_t.cluster_centers_[:,1],color='black',marker='+',s=200)\n",
					"plt.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dtest[\"ADRS\"]=0"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dtest[\"ADRS\"]=dtest[\"AD_D\"].apply(lambda x : 1 if (x==-1) else 0 )"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"for i in range(0,30):\n",
					"  if(dtest[dtest[\"C1_KM\"]==i][['AD_D']].value_counts().index[0][0]==-1):\n",
					"    if(int(len(dtest[dtest[\"C1_KM\"]==i][['AD_D']])/2)<int(dtest[dtest[\"C1_KM\"]==i][['AD_D']].value_counts().values[0])):\n",
					"      #print(i,dtest[dtest[\"C1_KM\"]==i][['AD_D']].value_counts().values[0],int((len(dtest[dtest[\"C1_KM\"]==i][['AD_D']])/2)/2))      \n",
					"      dtest.loc[dtest['C1_KM']==i,'ADRS']=1              "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"for i in range(0,10):\n",
					"  if(dtest[dtest[\"C2_KM\"]==i][['AD_T']].value_counts().index[0][0]==-1):\n",
					"    if(int(len(dtest[dtest[\"C2_KM\"]==i][['AD_T']])/2)<int(dtest[dtest[\"C2_KM\"]==i][['AD_T']].value_counts().values[0])):\n",
					"      #print(i,dtest[dtest[\"C2_KM\"]==i][['AD_T']].value_counts().values[0],int((len(dtest[dtest[\"C2_KM\"]==i][['AD_T']])/2)/2))      \n",
					"      dtest.loc[dtest['C2_KM']==i,'ADRS']=1"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dtest[\"ADRS\"]=dtest[[\"AD_T\",\"ADRS\"]].apply(lambda x : 1 if x[0]==-1 else x[1] ,axis=1)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"plt.scatter(pcd[:,0],pcd[:,1],c=dtest['ADRS'].values,cmap='rainbow',marker='o')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dtest['ADRS'].value_counts()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dtest.columns"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dfin=dtest[['Claim_FIR_No', 'Claim_Type', 'Hospitalization_Type','Class_of_Accommodation', 'Procedure', 'Diagnosis', 'PolicyNo',\n",
					"       'Policy_Type', 'member_code', 'Gender', 'Age', 'Hospital', 'Corporate', 'Length_of_Stay', 'Claim_Amount',\n",
					"       'HospitalCity', 'HospitalState', 'MemberCity','MemberState','Age_group', 'Claim_amt_grp', 'day', 'year', 'month', 'ADRS']]"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"dfin.shape"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
					"SDF=spark.createDataFrame(dfin) \n",
					"SDF.printSchema()\n",
					"#SDF.show()"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.core import Experiment, Workspace, Dataset, Datastore\n",
					"from azureml.data.dataset_factory import TabularDatasetFactory\n",
					"from azureml.core.authentication import ServicePrincipalAuthentication"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"subscription_id = dbutils.secrets.get(scope = \"ai-ml-workathon-scope\", key = \"SubscriptionId\") \n",
					"resource_group = \"uiap-d-ci-claimfraudanalytics\"\n",
					"workspace_name = \"ai-ml-amlworkspace\"\n",
					"experiment_name = \"sampleclaimfraud\"\n",
					"\n",
					"sp = ServicePrincipalAuthentication(tenant_id=dbutils.secrets.get(scope = \"ai-ml-workathon-scope\", key = \"TenantId\"), # tenantID\n",
					"                                    service_principal_id=dbutils.secrets.get(scope = \"ai-ml-workathon-scope\", key = \"ClientId\"), # clientId\n",
					"                                    service_principal_password=dbutils.secrets.get(scope = \"ai-ml-workathon-scope\", key = \"ClientSecret\")) # clientSecret\n",
					"\n",
					"ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name,auth=sp)\n",
					"\n",
					"datastore = Datastore.get_default(ws)\n",
					"dataset = TabularDatasetFactory.register_spark_dataframe(SDF, datastore, name = 'claiml-labelled-dataset', )"
				],
				"execution_count": null
			}
		]
	}
}